{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from modules.unet import UNet\n",
    "from modules.unet import dice_loss, dice_coeff, multiclass_dice_coeff\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from IPython.display import clear_output\n",
    "from torchmetrics import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore \n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "\n",
    "print('imported')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n",
      "Device name: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "# working device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Selected device: {}'.format(device))\n",
    "print('Device name: {}'.format(torch.cuda.get_device_name(device)))\n",
    "\n",
    "# other cosntants\n",
    "# you can modify from here: and only modify whats possible to modify (for backbones and optimizers)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 40\n",
    "IMG_SIZE = (128, 128)\n",
    "LEARNING_RATE = 0.0001\n",
    "IN_CHANNELS = 1\n",
    "\n",
    "class IdentityDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape: (426, 2, 128, 128) - max: 1.0\n",
      "test_data.shape: (609, 2, 128, 128) - max: 1.0\n",
      "fake_data.shape: (2000, 2, 128, 128) - max: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "# real_data = np.load(os.path.join(DATA_DIR, 'seg_train_data.npy'))\n",
    "# real_data = np.load('./data/brats_preprocessed.npy') # 3D data\n",
    "real_data = np.load('data/hecktor_data_pet.npy')\n",
    "train_data, test_data = real_data[:20], real_data[30:60]  # 30 samples for training, 30 for testing\n",
    "\n",
    "def process_data(real_data):\n",
    "    real_data = real_data.transpose(0, 4, 1, 2, 3)\n",
    "    real_data = real_data.reshape(-1, 2, 128, 128)\n",
    "\n",
    "    # drop empty masks\n",
    "    real_data = real_data[real_data[:, 1, ...].sum(axis=(1, 2)) > 0]\n",
    "\n",
    "    # normalize\n",
    "    for idx in range(real_data.shape[0]):\n",
    "        real_data[idx, 0] = (real_data[idx, 0] - real_data[idx, 0].min()) / (real_data[idx, 0].max() - real_data[idx, 0].min())\n",
    "\n",
    "    return real_data\n",
    "\n",
    "train_data = process_data(train_data)\n",
    "test_data = process_data(test_data)\n",
    "\n",
    "fake_data = np.load('data/generated_AHVAE_128_128_ds - hecktor_mask - True.npy')\n",
    "# fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "\n",
    "print('train_data.shape: {} - max: {}'.format(train_data.shape, train_data.max()))\n",
    "print('test_data.shape: {} - max: {}'.format(test_data.shape, test_data.max()))\n",
    "print('fake_data.shape: {} - max: {}'.format(fake_data.shape, fake_data.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images.shape: torch.Size([425, 1, 128, 128])\n",
      "train_masks.shape: torch.Size([425, 1, 128, 128])\n",
      "test_images.shape: torch.Size([609, 1, 128, 128])\n",
      "test_masks.shape: torch.Size([609, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# combining fake and real training data\n",
    "n_real, n_fake = -1, 0\n",
    "train_images = np.concatenate((train_data[:n_real, 0], fake_data[:n_fake, 0]))\n",
    "train_masks = np.concatenate((train_data[:n_real, 1], fake_data[:n_fake, 1]))\n",
    "\n",
    "test_images, test_masks = test_data[:, 0], test_data[:, 1]\n",
    "\n",
    "# shuffling the data\n",
    "rand_idx = np.arange(train_images.__len__())\n",
    "np.random.shuffle(rand_idx)\n",
    "train_images = train_images[rand_idx]\n",
    "train_masks = train_masks[rand_idx]\n",
    "\n",
    "# converting to torch tensors\n",
    "train_images = torch.from_numpy(train_images).type(torch.float32)\n",
    "train_masks = torch.from_numpy(train_masks).type(torch.float32)\n",
    "test_images = torch.from_numpy(test_images).type(torch.float32)\n",
    "test_masks = torch.from_numpy(test_masks).type(torch.float32)\n",
    "\n",
    "# unsqueeze the masks & images\n",
    "train_images = train_images.unsqueeze(1)\n",
    "train_masks = train_masks.unsqueeze(1).round()\n",
    "test_images = test_images.unsqueeze(1)\n",
    "test_masks = test_masks.unsqueeze(1)\n",
    "\n",
    "print('train_images.shape: {}'.format(train_images.shape))\n",
    "print('train_masks.shape: {}'.format(train_masks.shape))\n",
    "print('test_images.shape: {}'.format(test_images.shape))\n",
    "print('test_masks.shape: {}'.format(test_masks.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample images:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_sample = 5\n",
    "rand_idx = np.random.randint(0, train_images.shape[0], n_sample)\n",
    "print('Sample images:')\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(torch.hstack([img for img in train_images[rand_idx, 0]]), cmap='hot')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(torch.hstack([img for img in train_masks[rand_idx, 0]]), cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        model,\n",
    "        dataloader,\n",
    "        n_classes = 1,\n",
    "        epochs: int = 5,\n",
    "        learning_rate: float = 1e-5,\n",
    "        amp: bool = False,\n",
    "        weight_decay: float = 1e-8,\n",
    "        momentum: float = 0.999,\n",
    "        gradient_clipping: float = 2.0,\n",
    "        verbose = True\n",
    "):\n",
    "\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(),\n",
    "                              lr=learning_rate, weight_decay=weight_decay, momentum=momentum, foreach=True)\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, 1.0, 0.01, epochs)\n",
    "    grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "    # criterion = nn.CrossEntropyLoss() if n_classes > 1 else nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    train_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        running_total_loss = 0.0\n",
    "\n",
    "        progress = None\n",
    "        if verbose: progress = tqdm(dataloader, position=0, leave=True)\n",
    "        for i, (images, masks) in enumerate(dataloader):\n",
    "            images, masks = images.to(device, dtype=torch.float32), masks.to(device, dtype=torch.long)\n",
    "\n",
    "            with torch.autocast(device.type, enabled=amp):\n",
    "                masks_pred = model(images)\n",
    "                if n_classes == 1:\n",
    "                    # loss = criterion(masks_pred.squeeze(1), masks.squeeze(1).float())\n",
    "                    loss = dice_loss(torch.sigmoid(masks_pred.squeeze(1)), masks.squeeze(1).float(), multiclass=False)\n",
    "                else:\n",
    "                    # loss = criterion(masks_pred, masks)\n",
    "                    loss = dice_loss(\n",
    "                        F.softmax(masks_pred, dim=1).float(),\n",
    "                        F.one_hot(masks, n_classes).permute(0, 3, 1, 2).float(),\n",
    "                        multiclass=True\n",
    "                    )\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                grad_scaler.scale(loss).backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n",
    "                grad_scaler.step(optimizer)\n",
    "                grad_scaler.update()\n",
    "                # scheduler.step()\n",
    "\n",
    "            running_total_loss += loss.item()\n",
    "\n",
    "            if verbose:\n",
    "                with torch.no_grad():\n",
    "                    progress.update(1)\n",
    "                    progress.set_description('Epoch: {:5}/{} - Actual loss: {:.4f}'.format(\n",
    "                        epoch + 1, EPOCHS, running_total_loss / (i + 1)\n",
    "                    ))\n",
    "\n",
    "        train_loss.append(running_total_loss / len(dataloader))\n",
    "\n",
    "        if verbose: progress.close()\n",
    "        del progress\n",
    "\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return model, train_loss\n",
    "\n",
    "def evaluate_model(model, test_loader, n_classes, device='cuda', verbose=True, **kwargs): \n",
    "    if verbose:\n",
    "        print('--------------------------------------------')\n",
    "        print('Test metrics (on test set)')\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    #criterion\n",
    "    criterion = nn.CrossEntropyLoss() if n_classes > 1 else nn.BCEWithLogitsLoss()\n",
    "    fn = dice_coeff if n_classes == 1 else multiclass_dice_coeff\n",
    "\n",
    "    running_dice_score, running_ce_loss = 0.0, 0.0\n",
    "    predicted_masks = []\n",
    "    \n",
    "    # computing predictions\n",
    "    progress = None\n",
    "    if verbose: progress = tqdm(test_loader, position=0, leave=True)\n",
    "    for i, (images, masks) in enumerate(test_loader):\n",
    "        # images, masks = images.to(device, dtype=torch.float32), masks.to(device, dtype=torch.long)\n",
    "        masks_pred = model(images.to(device, dtype=torch.float32))\n",
    "        masks_pred = masks_pred.detach().cpu()\n",
    "        if n_classes == 1:\n",
    "            running_ce_loss += criterion(masks_pred.squeeze(1), masks.squeeze(1).float())\n",
    "            running_dice_score += fn(torch.sigmoid(masks_pred.squeeze(1)), masks.squeeze(1).float(), reduce_batch_first=True)\n",
    "        else:\n",
    "            running_ce_loss += criterion(masks_pred, masks)\n",
    "            running_dice_score += fn(\n",
    "                F.softmax(masks_pred, dim=1).float(),\n",
    "                F.one_hot(masks, n_classes).permute(0, 3, 1, 2).float(),\n",
    "                multiclass=True\n",
    "            )\n",
    "\n",
    "        if verbose: progress.update(1)\n",
    "\n",
    "    # computing main metrics\n",
    "    dice_coefficient = running_dice_score / len(test_loader)\n",
    "    ce_loss = running_ce_loss / len(test_loader)\n",
    "\n",
    "    # adding scores to the history\n",
    "    stats = {}\n",
    "    stats['dice_coeff'] = dice_coefficient\n",
    "    stats['ce_loss'] = ce_loss\n",
    "    stats['dice_loss'] = 1 - dice_coefficient\n",
    "\n",
    "    log = \"\"\"\n",
    "    --------------------------------------------\n",
    "    Total loss (Dice loss + CE loss): {:.3f} \n",
    "    Dice loss: {:.3f} \n",
    "    CE loss: {:.3f} \n",
    "    Dice coefficient: {:.3f} \n",
    "    --------------------------------------------\n",
    "    \"\"\".format(\n",
    "        1 - dice_coefficient + ce_loss, 1 - dice_coefficient, ce_loss, dice_coefficient\n",
    "    )\n",
    "\n",
    "    if verbose: print(log)\n",
    "\n",
    "    return stats, predicted_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess(n_real, n_fake, n_iters, amp, epochs, factor=None):\n",
    "    # combining fake and real training data\n",
    "    n_real, n_fake = n_real, n_fake\n",
    "    train_images = np.concatenate((train_data[:n_real, 0], fake_data[:n_fake, 0]))\n",
    "    train_masks = np.concatenate((train_data[:n_real, 1], fake_data[:n_fake, 1]))\n",
    "\n",
    "    train_images = torch.from_numpy(train_images).type(torch.float32).unsqueeze(1)\n",
    "    train_masks = torch.from_numpy(train_masks).type(torch.float32).unsqueeze(1).round()\n",
    "\n",
    "    test_images, test_masks = test_data[:, 0], test_data[:, 1]\n",
    "    test_images = torch.from_numpy(test_images).type(torch.float32).unsqueeze(1)\n",
    "    test_masks = torch.from_numpy(test_masks).type(torch.float32).unsqueeze(1).round()\n",
    "\n",
    "    if factor is not None:\n",
    "        data = torch.hstack((train_images, train_masks))\n",
    "\n",
    "        train_transforms = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=[-90, 90]),\n",
    "        ])\n",
    "\n",
    "        data = data.repeat(factor, 1, 1, 1)\n",
    "        data = torch.stack([train_transforms(data[i]) for i in range(len(data))])\n",
    "\n",
    "        train_images = data[:, 0, None]\n",
    "        train_masks = data[:, 1, None]\n",
    "\n",
    "\n",
    "    # shuffling the data\n",
    "    rand_idx = np.arange(train_images.__len__())\n",
    "    np.random.shuffle(rand_idx)\n",
    "    train_images = train_images[rand_idx]\n",
    "    train_masks = train_masks[rand_idx]\n",
    "\n",
    "\n",
    "    print('train_images.shape: {}'.format(train_images.shape))\n",
    "    print('train_masks.shape: {}'.format(train_masks.shape))\n",
    "    print('test_images.shape: {}'.format(test_images.shape))\n",
    "    print('test_masks.shape: {}'.format(test_masks.shape))\n",
    "\n",
    "\n",
    "    stats = []\n",
    "    for _ in tqdm(range(n_iters), position=0, leave=True):\n",
    "        train_loader = torch.utils.data.DataLoader(IdentityDataset(train_images, train_masks), batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "        test_loader = torch.utils.data.DataLoader(IdentityDataset(test_images, test_masks), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        model = UNet(n_channels=IN_CHANNELS, n_classes=1).to(device)\n",
    "\n",
    "        model, _ = train_model(\n",
    "            model, train_loader, n_classes=1, epochs=epochs, learning_rate=LEARNING_RATE, amp=amp, verbose=False\n",
    "        )\n",
    "\n",
    "        s, _ = evaluate_model(model, test_loader, n_classes=1, verbose=False)\n",
    "        stats.append(s)\n",
    "    \n",
    "    print('mean {} - std: {}'.format(np.mean([s['dice_coeff'] for s in stats]), np.std([s['dice_coeff'] for s in stats])))\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images.shape: torch.Size([2850, 1, 128, 128])\n",
      "train_masks.shape: torch.Size([2850, 1, 128, 128])\n",
      "test_images.shape: torch.Size([609, 1, 128, 128])\n",
      "test_masks.shape: torch.Size([609, 1, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [33:49<00:00, 405.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 0.5408599376678467 - std: 0.03649711608886719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'dice_coeff': tensor(0.5055),\n",
       "  'ce_loss': tensor(20.4551),\n",
       "  'dice_loss': tensor(0.4945)},\n",
       " {'dice_coeff': tensor(0.5566),\n",
       "  'ce_loss': tensor(24.6629),\n",
       "  'dice_loss': tensor(0.4434)},\n",
       " {'dice_coeff': tensor(0.6027),\n",
       "  'ce_loss': tensor(11.9655),\n",
       "  'dice_loss': tensor(0.3973)},\n",
       " {'dice_coeff': tensor(0.5346),\n",
       "  'ce_loss': tensor(29.1716),\n",
       "  'dice_loss': tensor(0.4654)},\n",
       " {'dice_coeff': tensor(0.5048),\n",
       "  'ce_loss': tensor(36.4429),\n",
       "  'dice_loss': tensor(0.4952)}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assess(n_real=-1, n_fake=1000, n_iters=5, amp=False, epochs=40, factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images.shape: torch.Size([4275, 1, 128, 128])\n",
      "train_masks.shape: torch.Size([4275, 1, 128, 128])\n",
      "test_images.shape: torch.Size([609, 1, 128, 128])\n",
      "test_masks.shape: torch.Size([609, 1, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [48:12<00:00, 578.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 0.531981885433197 - std: 0.03459751605987549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'dice_coeff': tensor(0.5294),\n",
       "  'ce_loss': tensor(55.1747),\n",
       "  'dice_loss': tensor(0.4706)},\n",
       " {'dice_coeff': tensor(0.5867),\n",
       "  'ce_loss': tensor(27.1487),\n",
       "  'dice_loss': tensor(0.4133)},\n",
       " {'dice_coeff': tensor(0.5456),\n",
       "  'ce_loss': tensor(30.5149),\n",
       "  'dice_loss': tensor(0.4544)},\n",
       " {'dice_coeff': tensor(0.4812),\n",
       "  'ce_loss': tensor(39.0400),\n",
       "  'dice_loss': tensor(0.5188)},\n",
       " {'dice_coeff': tensor(0.5171),\n",
       "  'ce_loss': tensor(33.6175),\n",
       "  'dice_loss': tensor(0.4829)}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assess(n_real=-1, n_fake=1000, n_iters=5, amp=False, epochs=40, factor=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images.shape: torch.Size([7125, 1, 128, 128])\n",
      "train_masks.shape: torch.Size([7125, 1, 128, 128])\n",
      "test_images.shape: torch.Size([609, 1, 128, 128])\n",
      "test_masks.shape: torch.Size([609, 1, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [1:22:06<00:00, 985.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 0.5573706030845642 - std: 0.011137989349663258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'dice_coeff': tensor(0.5722),\n",
       "  'ce_loss': tensor(39.5178),\n",
       "  'dice_loss': tensor(0.4278)},\n",
       " {'dice_coeff': tensor(0.5645),\n",
       "  'ce_loss': tensor(26.5413),\n",
       "  'dice_loss': tensor(0.4355)},\n",
       " {'dice_coeff': tensor(0.5535),\n",
       "  'ce_loss': tensor(19.5889),\n",
       "  'dice_loss': tensor(0.4465)},\n",
       " {'dice_coeff': tensor(0.5391),\n",
       "  'ce_loss': tensor(29.0793),\n",
       "  'dice_loss': tensor(0.4609)},\n",
       " {'dice_coeff': tensor(0.5575),\n",
       "  'ce_loss': tensor(38.0603),\n",
       "  'dice_loss': tensor(0.4425)}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assess(n_real=-1, n_fake=1000, n_iters=5, amp=False, epochs=40, factor=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_samples(real, fake, verbose=True, device='cuda', **kwargs): \n",
    "    if verbose:\n",
    "        print('--------------------------------------------')\n",
    "        print('Test metrics (on test set)')\n",
    "\n",
    "    # recon = []\n",
    "    # # computing predictions\n",
    "    # for i, x in enumerate(tqdm(test_loader, position=0, leave=True)):\n",
    "    #     x = x.to(device, dtype=torch.float32)\n",
    "    #     output = model(x)\n",
    "    #     stats = model.loss_function(output['x'], x, output['mu'], output['logvar'])\n",
    "    #     recon.append(output['x'])\n",
    "    stats = dict()\n",
    "\n",
    "    # # computing main metrics\n",
    "    # elbo = stats['loss'].item()\n",
    "    # recon_loss = stats['recon_loss'].item()\n",
    "    # kld_loss = stats['kld_loss'].item()\n",
    "\n",
    "    # real = torch.vstack([img for img in test_loader]).to(device, dtype=torch.float32)\n",
    "    # recon = torch.vstack([img for img in recon]).to(device, dtype=torch.float32)\n",
    "    # fake = model.sample(n_samples=len(test_loader))\n",
    "\n",
    "    print('Evaluation on {} real/fake/recon images'.format(len(real)))\n",
    "\n",
    "    # computing ssim and psnr from outputs\n",
    "    psnr = PeakSignalNoiseRatio().to(device)\n",
    "    psnr_score = psnr(fake, real)\n",
    "    ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "    ssim_score = ssim(fake, real)\n",
    "    \n",
    "    if real.shape[1] == 1:\n",
    "        real = real.repeat(1, 3, 1, 1)\n",
    "        fake = fake.repeat(1, 3, 1, 1)\n",
    "        # recon = recon.repeat(1, 3, 1, 1)\n",
    "\n",
    "    fid = FrechetInceptionDistance(normalize=True).to(device)\n",
    "    fid.update(real, real=True)\n",
    "    fid.update(fake, real=False)\n",
    "    fid_score = fid.compute()\n",
    "    incep = InceptionScore(normalize=True).to(device)\n",
    "    incep.update(fake)\n",
    "    mean_is_score, std_is_score = incep.compute()\n",
    "    is_score = torch.exp(mean_is_score) / torch.sqrt(std_is_score)\n",
    "    lpips = LearnedPerceptualImagePatchSimilarity(net_type='vgg', normalize=True).to(device)\n",
    "    lpips_score = lpips(real, fake)\n",
    "\n",
    "    # adding scores to the history\n",
    "    stats['psnr'] = psnr_score\n",
    "    stats['ssim'] = ssim_score\n",
    "    stats['fid'] = fid_score\n",
    "    stats['is'] = (mean_is_score, std_is_score, is_score)\n",
    "    stats['lpips'] = lpips_score\n",
    "\n",
    "    log = \"\"\"\n",
    "    --------------------------------------------\n",
    "    Total loss (ELBO): {:.3f} \n",
    "    Mean squared error: {:.3f} \n",
    "    Kullback-Leibler divergence: {:.3f} \n",
    "    PSNR: {:.3f} \n",
    "    SSIM: {:.3f} \n",
    "    FID: {:.3f} \n",
    "    IS: (mean) {:.3f} (std) {:.3f} : {:.3f} \n",
    "    LPIPS: {:.3f} \n",
    "    kwargs: {}\n",
    "    --------------------------------------------\n",
    "    \"\"\".format(\n",
    "        0, 0, 0, psnr_score, ssim_score, fid_score, mean_is_score, \n",
    "        std_is_score, is_score, lpips_score, kwargs\n",
    "    )\n",
    "\n",
    "    if verbose: print(log)\n",
    "\n",
    "    return stats\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HECKTOR dHVAE-HECKTOR (original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 609 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 \n",
      "    Mean squared error: 0.000 \n",
      "    Kullback-Leibler divergence: 0.000 \n",
      "    PSNR: 19.115 \n",
      "    SSIM: 0.600 \n",
      "    FID: 74.428 \n",
      "    IS: (mean) 1.027 (std) 0.020 : 19.854 \n",
      "    LPIPS: 0.310 \n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/dHVAE-HECKTOR (original).npy')\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AHVAE - HECKTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 609 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 \n",
      "    Mean squared error: 0.000 \n",
      "    Kullback-Leibler divergence: 0.000 \n",
      "    PSNR: 19.570 \n",
      "    SSIM: 0.580 \n",
      "    FID: 54.008 \n",
      "    IS: (mean) 1.063 (std) 0.023 : 19.259 \n",
      "    LPIPS: 0.309 \n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/generated_AHVAE_128_128_ds - hecktor_mask - True.npy')\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE (HECKTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 609 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 \n",
      "    Mean squared error: 0.000 \n",
      "    Kullback-Leibler divergence: 0.000 \n",
      "    PSNR: 19.038 \n",
      "    SSIM: 0.567 \n",
      "    FID: 90.208 \n",
      "    IS: (mean) 1.037 (std) 0.024 : 18.132 \n",
      "    LPIPS: 0.338 \n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/generated_VAE_128_128_ds=hecktor, mask=True.npy')\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSGAN (HECKTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 609 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 \n",
      "    Mean squared error: 0.000 \n",
      "    Kullback-Leibler divergence: 0.000 \n",
      "    PSNR: 18.440 \n",
      "    SSIM: 0.488 \n",
      "    FID: 189.753 \n",
      "    IS: (mean) 1.773 (std) 0.126 : 16.555 \n",
      "    LPIPS: 0.459 \n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/generated_LSGAN_128_128_ds - hecktor_mask - True.npy')\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BRATS - AHVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 \n",
      "    Mean squared error: 0.000 \n",
      "    Kullback-Leibler divergence: 0.000 \n",
      "    PSNR: 15.582 \n",
      "    SSIM: 0.647 \n",
      "    FID: 94.679 \n",
      "    IS: (mean) 1.569 (std) 0.054 : 20.587 \n",
      "    LPIPS: 0.227 \n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/generated_AHVAE_128_128_ds - brats_mask - True.npy')\n",
    "fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brats - VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 \n",
      "    Mean squared error: 0.000 \n",
      "    Kullback-Leibler divergence: 0.000 \n",
      "    PSNR: 15.181 \n",
      "    SSIM: 0.644 \n",
      "    FID: 106.276 \n",
      "    IS: (mean) 1.730 (std) 0.144 : 14.853 \n",
      "    LPIPS: 0.273 \n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/generated_VAE_128_128_ds=brats, mask=True.npy')\n",
    "fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HVAE BRATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 \n",
      "    Mean squared error: 0.000 \n",
      "    Kullback-Leibler divergence: 0.000 \n",
      "    PSNR: 15.336 \n",
      "    SSIM: 0.655 \n",
      "    FID: 124.750 \n",
      "    IS: (mean) 1.312 (std) 0.057 : 15.542 \n",
      "    LPIPS: 0.248 \n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/generated_HVAE_128_128_ds - brats, mask - True, dt - 0.1.npy')\n",
    "fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSGAN BRATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 500 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 \n",
      "    Mean squared error: 0.000 \n",
      "    Kullback-Leibler divergence: 0.000 \n",
      "    PSNR: 14.579 \n",
      "    SSIM: 0.442 \n",
      "    FID: 158.289 \n",
      "    IS: (mean) 1.856 (std) 0.066 : 24.832 \n",
      "    LPIPS: 0.473 \n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_images = np.load('data/generated_LSGAN_128_128_ds - brats_mask - True.npy')\n",
    "fake_images = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images[:500].to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRATS - dHVAE old training with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 \n",
      "    Mean squared error: 0.000 \n",
      "    Kullback-Leibler divergence: 0.000 \n",
      "    PSNR: 15.614 \n",
      "    SSIM: 0.647 \n",
      "    FID: 92.488 \n",
      "    IS: (mean) 1.582 (std) 0.052 : 21.427 \n",
      "    LPIPS: 0.227 \n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/dHVAE-BRATS (original).npy')\n",
    "fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dHVAE - BRATS (0.01) = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 \n",
      "    Mean squared error: 0.000 \n",
      "    Kullback-Leibler divergence: 0.000 \n",
      "    PSNR: 13.795 \n",
      "    SSIM: 0.578 \n",
      "    FID: 123.964 \n",
      "    IS: (mean) 1.719 (std) 0.086 : 19.076 \n",
      "    LPIPS: 0.283 \n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/dHVAE-BRATS (0.05).npy')\n",
    "fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRATS - dHVAE (0.001) = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 \n",
      "    Mean squared error: 0.000 \n",
      "    Kullback-Leibler divergence: 0.000 \n",
      "    PSNR: 15.409 \n",
      "    SSIM: 0.632 \n",
      "    FID: 126.503 \n",
      "    IS: (mean) 1.707 (std) 0.055 : 23.470 \n",
      "    LPIPS: 0.233 \n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/dHVAE-BRATS (0.05).npy')\n",
    "# fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dHVAE - BRATS (0.1) = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 \n",
      "    Mean squared error: 0.000 \n",
      "    Kullback-Leibler divergence: 0.000 \n",
      "    PSNR: 15.208 \n",
      "    SSIM: 0.633 \n",
      "    FID: 147.453 \n",
      "    IS: (mean) 1.671 (std) 0.049 : 24.154 \n",
      "    LPIPS: 0.235 \n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/dHVAE-BRATS (0.1).npy')\n",
    "# fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dHVAE - BRATS (0.01) Fethi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 \n",
      "    Mean squared error: 0.000 \n",
      "    Kullback-Leibler divergence: 0.000 \n",
      "    PSNR: 15.622 \n",
      "    SSIM: 0.627 \n",
      "    FID: 88.299 \n",
      "    IS: (mean) 1.817 (std) 0.075 : 22.525 \n",
      "    LPIPS: 0.229 \n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/dHVAE-BRATS (0.01) Fethi.npy')\n",
    "# fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HVAE + Percep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n",
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 \n",
      "    Mean squared error: 0.000 \n",
      "    Kullback-Leibler divergence: 0.000 \n",
      "    PSNR: 14.923 \n",
      "    SSIM: 0.637 \n",
      "    FID: 104.767 \n",
      "    IS: (mean) 1.600 (std) 0.052 : 21.715 \n",
      "    LPIPS: 0.225 \n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/HVAE-Percep-BRATS.npy')\n",
    "# fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HVAE + Disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 \n",
      "    Mean squared error: 0.000 \n",
      "    Kullback-Leibler divergence: 0.000 \n",
      "    PSNR: 15.102 \n",
      "    SSIM: 0.633 \n",
      "    FID: 147.954 \n",
      "    IS: (mean) 1.681 (std) 0.085 : 18.456 \n",
      "    LPIPS: 0.237 \n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/HVAE-Disc-Brats.npy')\n",
    "# fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fae847446f25d1d5cccaa632528b81cb53ce4d6408a7df79225531d1adf33a86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
