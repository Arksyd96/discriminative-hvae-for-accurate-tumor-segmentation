{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from modules.unet import UNet\n",
    "from modules.unet import dice_loss, dice_coeff, multiclass_dice_coeff\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from IPython.display import clear_output\n",
    "from torchmetrics import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore \n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "\n",
    "print('imported')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n",
      "Device name: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "# working device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Selected device: {}'.format(device))\n",
    "print('Device name: {}'.format(torch.cuda.get_device_name(device)))\n",
    "\n",
    "# other cosntants\n",
    "# you can modify from here: and only modify whats possible to modify (for backbones and optimizers)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 40\n",
    "IMG_SIZE = (128, 128)\n",
    "LEARNING_RATE = 0.0001\n",
    "IN_CHANNELS = 1\n",
    "\n",
    "class IdentityDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape: (266, 2, 128, 128) - max: 1.0\n",
      "test_data.shape: (828, 2, 128, 128) - max: 1.0\n",
      "fake_data.shape: (2000, 2, 128, 128) - max: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "# real_data = np.load(os.path.join(DATA_DIR, 'seg_train_data.npy'))\n",
    "# real_data = np.load('./data/brats_preprocessed.npy') # 3D data\n",
    "real_data = np.load('data/brats_preprocessed.npy')\n",
    "train_data, test_data = real_data[:10], real_data[100:130]  # 30 samples for training, 30 for testing\n",
    "\n",
    "def process_data(real_data):\n",
    "    real_data = real_data.transpose(0, 4, 1, 2, 3)\n",
    "    real_data = real_data.reshape(-1, 2, 128, 128)\n",
    "\n",
    "    # drop empty masks\n",
    "    real_data = real_data[real_data[:, 1, ...].sum(axis=(1, 2)) > 0]\n",
    "\n",
    "    # normalize\n",
    "    for idx in range(real_data.shape[0]):\n",
    "        real_data[idx, 0] = (real_data[idx, 0] - real_data[idx, 0].min()) / (real_data[idx, 0].max() - real_data[idx, 0].min())\n",
    "\n",
    "    return real_data\n",
    "\n",
    "train_data = process_data(train_data)\n",
    "test_data = process_data(test_data)\n",
    "\n",
    "fake_data = np.load('data/generated_AHVAE_128_128_ds - brats_mask - True.npy')\n",
    "# fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "\n",
    "print('train_data.shape: {} - max: {}'.format(train_data.shape, train_data.max()))\n",
    "print('test_data.shape: {} - max: {}'.format(test_data.shape, test_data.max()))\n",
    "print('fake_data.shape: {} - max: {}'.format(fake_data.shape, fake_data.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images.shape: torch.Size([2265, 1, 128, 128])\n",
      "train_masks.shape: torch.Size([2265, 1, 128, 128])\n",
      "test_images.shape: torch.Size([828, 1, 128, 128])\n",
      "test_masks.shape: torch.Size([828, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# combining fake and real training data\n",
    "n_real, n_fake = -1, 2000\n",
    "train_images = np.concatenate((train_data[:n_real, 0], fake_data[:n_fake, 0]))\n",
    "train_masks = np.concatenate((train_data[:n_real, 1], fake_data[:n_fake, 1]))\n",
    "\n",
    "test_images, test_masks = test_data[:, 0], test_data[:, 1]\n",
    "\n",
    "# shuffling the data\n",
    "rand_idx = np.arange(train_images.__len__())\n",
    "np.random.shuffle(rand_idx)\n",
    "train_images = train_images[rand_idx]\n",
    "train_masks = train_masks[rand_idx]\n",
    "\n",
    "# converting to torch tensors\n",
    "train_images = torch.from_numpy(train_images).type(torch.float32)\n",
    "train_masks = torch.from_numpy(train_masks).type(torch.float32)\n",
    "test_images = torch.from_numpy(test_images).type(torch.float32)\n",
    "test_masks = torch.from_numpy(test_masks).type(torch.float32)\n",
    "\n",
    "# unsqueeze the masks & images\n",
    "train_images = train_images.unsqueeze(1)\n",
    "train_masks = train_masks.unsqueeze(1).round()\n",
    "test_images = test_images.unsqueeze(1)\n",
    "test_masks = test_masks.unsqueeze(1)\n",
    "\n",
    "print('train_images.shape: {}'.format(train_images.shape))\n",
    "print('train_masks.shape: {}'.format(train_masks.shape))\n",
    "print('test_images.shape: {}'.format(test_images.shape))\n",
    "print('test_masks.shape: {}'.format(test_masks.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample images:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_sample = 5\n",
    "rand_idx = np.random.randint(0, train_images.shape[0], n_sample)\n",
    "print('Sample images:')\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(torch.hstack([img for img in train_images[rand_idx, 0]]), cmap='gray')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(torch.hstack([img for img in train_masks[rand_idx, 0]]), cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        model,\n",
    "        dataloader,\n",
    "        n_classes = 1,\n",
    "        epochs: int = 5,\n",
    "        learning_rate: float = 1e-5,\n",
    "        amp: bool = False,\n",
    "        weight_decay: float = 1e-8,\n",
    "        momentum: float = 0.999,\n",
    "        gradient_clipping: float = 2.0,\n",
    "        verbose = True\n",
    "):\n",
    "\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(),\n",
    "                              lr=learning_rate, weight_decay=weight_decay, momentum=momentum, foreach=True)\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, 1.0, 0.01, epochs)\n",
    "    grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "    # criterion = nn.CrossEntropyLoss() if n_classes > 1 else nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    train_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        running_total_loss = 0.0\n",
    "\n",
    "        progress = None\n",
    "        if verbose: progress = tqdm(dataloader, position=0, leave=True)\n",
    "        for i, (images, masks) in enumerate(dataloader):\n",
    "            images, masks = images.to(device, dtype=torch.float32), masks.to(device, dtype=torch.long)\n",
    "\n",
    "            with torch.autocast(device.type, enabled=amp):\n",
    "                masks_pred = model(images)\n",
    "                if n_classes == 1:\n",
    "                    # loss = criterion(masks_pred.squeeze(1), masks.squeeze(1).float())\n",
    "                    loss = dice_loss(torch.sigmoid(masks_pred.squeeze(1)), masks.squeeze(1).float(), multiclass=False)\n",
    "                else:\n",
    "                    # loss = criterion(masks_pred, masks)\n",
    "                    loss = dice_loss(\n",
    "                        F.softmax(masks_pred, dim=1).float(),\n",
    "                        F.one_hot(masks, n_classes).permute(0, 3, 1, 2).float(),\n",
    "                        multiclass=True\n",
    "                    )\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                grad_scaler.scale(loss).backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n",
    "                grad_scaler.step(optimizer)\n",
    "                grad_scaler.update()\n",
    "                # scheduler.step()\n",
    "\n",
    "            running_total_loss += loss.item()\n",
    "\n",
    "            if verbose:\n",
    "                with torch.no_grad():\n",
    "                    progress.update(1)\n",
    "                    progress.set_description('Epoch: {:5}/{} - Actual loss: {:.4f}'.format(\n",
    "                        epoch + 1, EPOCHS, running_total_loss / (i + 1)\n",
    "                    ))\n",
    "\n",
    "        train_loss.append(running_total_loss / len(dataloader))\n",
    "\n",
    "        if verbose: progress.close()\n",
    "        del progress\n",
    "\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return model, train_loss\n",
    "\n",
    "def evaluate_model(model, test_loader, n_classes, device='cuda', verbose=True, **kwargs): \n",
    "    if verbose:\n",
    "        print('--------------------------------------------')\n",
    "        print('Test metrics (on test set)')\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # criterion\n",
    "    criterion = nn.CrossEntropyLoss() if n_classes > 1 else nn.BCEWithLogitsLoss()\n",
    "    fn = dice_coeff if n_classes == 1 else multiclass_dice_coeff\n",
    "\n",
    "    running_dice_score, running_ce_loss = 0.0, 0.0\n",
    "    predicted_masks = []\n",
    "    \n",
    "    # computing predictions\n",
    "    progress = None\n",
    "    if verbose: progress = tqdm(test_loader, position=0, leave=True)\n",
    "    for i, (images, masks) in enumerate(test_loader):\n",
    "        # images, masks = images.to(device, dtype=torch.float32), masks.to(device, dtype=torch.long)\n",
    "        masks_pred = model(images.to(device, dtype=torch.float32))\n",
    "        masks_pred = masks_pred.detach().cpu()\n",
    "        if n_classes == 1:\n",
    "            running_ce_loss += criterion(masks_pred.squeeze(1), masks.squeeze(1).float())\n",
    "            running_dice_score += fn(torch.sigmoid(masks_pred.squeeze(1)), masks.squeeze(1).float(), reduce_batch_first=True)\n",
    "        else:\n",
    "            running_ce_loss += criterion(masks_pred, masks)\n",
    "            running_dice_score += fn(\n",
    "                F.softmax(masks_pred, dim=1).float(),\n",
    "                F.one_hot(masks, n_classes).permute(0, 3, 1, 2).float(),\n",
    "                multiclass=True\n",
    "            )\n",
    "\n",
    "        if verbose: progress.update(1)\n",
    "\n",
    "    # computing main metrics\n",
    "    dice_coefficient = running_dice_score / len(test_loader)\n",
    "    ce_loss = running_ce_loss / len(test_loader)\n",
    "\n",
    "    # adding scores to the history\n",
    "    stats = {}\n",
    "    stats['dice_coeff'] = dice_coefficient\n",
    "    stats['ce_loss'] = ce_loss\n",
    "    stats['dice_loss'] = 1 - dice_coefficient\n",
    "\n",
    "    log = \"\"\"\n",
    "    --------------------------------------------\n",
    "    Total loss (Dice loss + CE loss): {:.3f} ↓\n",
    "    Dice loss: {:.3f} ↓\n",
    "    CE loss: {:.3f} ↓\n",
    "    Dice coefficient: {:.3f} ↑\n",
    "    --------------------------------------------\n",
    "    \"\"\".format(\n",
    "        1 - dice_coefficient + ce_loss, 1 - dice_coefficient, ce_loss, dice_coefficient\n",
    "    )\n",
    "\n",
    "    if verbose: print(log)\n",
    "\n",
    "    return stats, predicted_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess(n_real, n_fake, n_iters, amp, epochs, factor=None):\n",
    "    # combining fake and real training data\n",
    "    n_real, n_fake = n_real, n_fake\n",
    "    train_images = np.concatenate((train_data[:n_real, 0], fake_data[:n_fake, 0]))\n",
    "    train_masks = np.concatenate((train_data[:n_real, 1], fake_data[:n_fake, 1]))\n",
    "\n",
    "    train_images = torch.from_numpy(train_images).type(torch.float32).unsqueeze(1)\n",
    "    train_masks = torch.from_numpy(train_masks).type(torch.float32).unsqueeze(1).round()\n",
    "\n",
    "    test_images, test_masks = test_data[:, 0], test_data[:, 1]\n",
    "    test_images = torch.from_numpy(test_images).type(torch.float32).unsqueeze(1)\n",
    "    test_masks = torch.from_numpy(test_masks).type(torch.float32).unsqueeze(1).round()\n",
    "\n",
    "    if factor is not None:\n",
    "        data = torch.hstack((train_images, train_masks))\n",
    "\n",
    "        train_transforms = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=[-90, 90]),\n",
    "        ])\n",
    "\n",
    "        data = data.repeat(factor, 1, 1, 1)\n",
    "        data = torch.stack([train_transforms(data[i]) for i in range(len(data))])\n",
    "\n",
    "        train_images = data[:, 0, None]\n",
    "        train_masks = data[:, 1, None]\n",
    "\n",
    "\n",
    "    # shuffling the data\n",
    "    rand_idx = np.arange(train_images.__len__())\n",
    "    np.random.shuffle(rand_idx)\n",
    "    train_images = train_images[rand_idx]\n",
    "    train_masks = train_masks[rand_idx]\n",
    "\n",
    "\n",
    "    print('train_images.shape: {}'.format(train_images.shape))\n",
    "    print('train_masks.shape: {}'.format(train_masks.shape))\n",
    "    print('test_images.shape: {}'.format(test_images.shape))\n",
    "    print('test_masks.shape: {}'.format(test_masks.shape))\n",
    "\n",
    "\n",
    "    stats = []\n",
    "    for _ in tqdm(range(n_iters), position=0, leave=True):\n",
    "        train_loader = torch.utils.data.DataLoader(IdentityDataset(train_images, train_masks), batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "        test_loader = torch.utils.data.DataLoader(IdentityDataset(test_images, test_masks), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        model = UNet(n_channels=IN_CHANNELS, n_classes=1).to(device)\n",
    "\n",
    "        model, _ = train_model(\n",
    "            model, train_loader, n_classes=1, epochs=epochs, learning_rate=LEARNING_RATE, amp=amp, verbose=False\n",
    "        )\n",
    "\n",
    "        s, _ = evaluate_model(model, test_loader, n_classes=1, verbose=False)\n",
    "        stats.append(s)\n",
    "    \n",
    "    print('mean {} - std: {}'.format(np.mean([s['dice_coeff'] for s in stats]), np.std([s['dice_coeff'] for s in stats])))\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images.shape: torch.Size([2265, 1, 128, 128])\n",
      "train_masks.shape: torch.Size([2265, 1, 128, 128])\n",
      "test_images.shape: torch.Size([828, 1, 128, 128])\n",
      "test_masks.shape: torch.Size([828, 1, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [24:49<00:00, 297.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 0.7909132838249207 - std: 0.009755819104611874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'dice_coeff': tensor(0.7951),\n",
       "  'ce_loss': tensor(15.5384),\n",
       "  'dice_loss': tensor(0.2049)},\n",
       " {'dice_coeff': tensor(0.8037),\n",
       "  'ce_loss': tensor(20.2442),\n",
       "  'dice_loss': tensor(0.1963)},\n",
       " {'dice_coeff': tensor(0.7860),\n",
       "  'ce_loss': tensor(22.2032),\n",
       "  'dice_loss': tensor(0.2140)},\n",
       " {'dice_coeff': tensor(0.7948),\n",
       "  'ce_loss': tensor(19.0245),\n",
       "  'dice_loss': tensor(0.2052)},\n",
       " {'dice_coeff': tensor(0.7749),\n",
       "  'ce_loss': tensor(27.4660),\n",
       "  'dice_loss': tensor(0.2251)}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assess(n_real=-1, n_fake=2000, n_iters=5, amp=False, epochs=40, factor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images.shape: torch.Size([4530, 1, 128, 128])\n",
      "train_masks.shape: torch.Size([4530, 1, 128, 128])\n",
      "test_images.shape: torch.Size([828, 1, 128, 128])\n",
      "test_masks.shape: torch.Size([828, 1, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [30:21<00:00, 364.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 0.7177838087081909 - std: 0.046756498515605927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'dice_coeff': tensor(0.6343),\n",
       "  'ce_loss': tensor(0.8222),\n",
       "  'dice_loss': tensor(0.3657)},\n",
       " {'dice_coeff': tensor(0.7053),\n",
       "  'ce_loss': tensor(0.2102),\n",
       "  'dice_loss': tensor(0.2947)},\n",
       " {'dice_coeff': tensor(0.7696),\n",
       "  'ce_loss': tensor(0.1542),\n",
       "  'dice_loss': tensor(0.2304)},\n",
       " {'dice_coeff': tensor(0.7486),\n",
       "  'ce_loss': tensor(0.7913),\n",
       "  'dice_loss': tensor(0.2514)},\n",
       " {'dice_coeff': tensor(0.7311),\n",
       "  'ce_loss': tensor(0.4123),\n",
       "  'dice_loss': tensor(0.2689)}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assess(n_real=-1, n_fake=2000, n_iters=5, amp=False, epochs=40, factor=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_samples(real, fake, verbose=True, device='cuda', **kwargs): \n",
    "    if verbose:\n",
    "        print('--------------------------------------------')\n",
    "        print('Test metrics (on test set)')\n",
    "\n",
    "    # recon = []\n",
    "    # # computing predictions\n",
    "    # for i, x in enumerate(tqdm(test_loader, position=0, leave=True)):\n",
    "    #     x = x.to(device, dtype=torch.float32)\n",
    "    #     output = model(x)\n",
    "    #     stats = model.loss_function(output['x'], x, output['mu'], output['logvar'])\n",
    "    #     recon.append(output['x'])\n",
    "    stats = dict()\n",
    "\n",
    "    # # computing main metrics\n",
    "    # elbo = stats['loss'].item()\n",
    "    # recon_loss = stats['recon_loss'].item()\n",
    "    # kld_loss = stats['kld_loss'].item()\n",
    "\n",
    "    # real = torch.vstack([img for img in test_loader]).to(device, dtype=torch.float32)\n",
    "    # recon = torch.vstack([img for img in recon]).to(device, dtype=torch.float32)\n",
    "    # fake = model.sample(n_samples=len(test_loader))\n",
    "\n",
    "    print('Evaluation on {} real/fake/recon images'.format(len(real)))\n",
    "\n",
    "    # computing ssim and psnr from outputs\n",
    "    psnr = PeakSignalNoiseRatio().to(device)\n",
    "    psnr_score = psnr(fake, real)\n",
    "    ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "    ssim_score = ssim(fake, real)\n",
    "    \n",
    "    if real.shape[1] == 1:\n",
    "        real = real.repeat(1, 3, 1, 1)\n",
    "        fake = fake.repeat(1, 3, 1, 1)\n",
    "        # recon = recon.repeat(1, 3, 1, 1)\n",
    "\n",
    "    fid = FrechetInceptionDistance(normalize=True).to(device)\n",
    "    fid.update(real, real=True)\n",
    "    fid.update(fake, real=False)\n",
    "    fid_score = fid.compute()\n",
    "    incep = InceptionScore(normalize=True).to(device)\n",
    "    incep.update(fake)\n",
    "    mean_is_score, std_is_score = incep.compute()\n",
    "    is_score = torch.exp(mean_is_score) / torch.sqrt(std_is_score)\n",
    "    lpips = LearnedPerceptualImagePatchSimilarity(net_type='vgg', normalize=True).to(device)\n",
    "    lpips_score = lpips(real, fake)\n",
    "\n",
    "    # adding scores to the history\n",
    "    stats['psnr'] = psnr_score\n",
    "    stats['ssim'] = ssim_score\n",
    "    stats['fid'] = fid_score\n",
    "    stats['is'] = (mean_is_score, std_is_score, is_score)\n",
    "    stats['lpips'] = lpips_score\n",
    "\n",
    "    log = \"\"\"\n",
    "    --------------------------------------------\n",
    "    Total loss (ELBO): {:.3f} ↓\n",
    "    Mean squared error: {:.3f} ↓\n",
    "    Kullback-Leibler divergence: {:.3f} ↓\n",
    "    PSNR: {:.3f} ↑\n",
    "    SSIM: {:.3f} ↑\n",
    "    FID: {:.3f} ↓\n",
    "    IS: (mean) {:.3f} (std) {:.3f} : {:.3f} ↑\n",
    "    LPIPS: {:.3f} ↓\n",
    "    kwargs: {}\n",
    "    --------------------------------------------\n",
    "    \"\"\".format(\n",
    "        0, 0, 0, psnr_score, ssim_score, fid_score, mean_is_score, \n",
    "        std_is_score, is_score, lpips_score, kwargs\n",
    "    )\n",
    "\n",
    "    if verbose: print(log)\n",
    "\n",
    "    return stats\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HECKTOR dHVAE-HECKTOR (original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 609 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 19.115 ↑\n",
      "    SSIM: 0.600 ↑\n",
      "    FID: 74.428 ↓\n",
      "    IS: (mean) 1.027 (std) 0.020 : 19.854 ↑\n",
      "    LPIPS: 0.310 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/dHVAE-HECKTOR (original).npy')\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AHVAE - HECKTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 609 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 19.570 ↑\n",
      "    SSIM: 0.580 ↑\n",
      "    FID: 54.008 ↓\n",
      "    IS: (mean) 1.063 (std) 0.023 : 19.259 ↑\n",
      "    LPIPS: 0.309 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/generated_AHVAE_128_128_ds - hecktor_mask - True.npy')\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE (HECKTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 609 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 19.038 ↑\n",
      "    SSIM: 0.567 ↑\n",
      "    FID: 90.208 ↓\n",
      "    IS: (mean) 1.037 (std) 0.024 : 18.132 ↑\n",
      "    LPIPS: 0.338 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/generated_VAE_128_128_ds=hecktor, mask=True.npy')\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSGAN (HECKTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 609 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 18.440 ↑\n",
      "    SSIM: 0.488 ↑\n",
      "    FID: 189.753 ↓\n",
      "    IS: (mean) 1.773 (std) 0.126 : 16.555 ↑\n",
      "    LPIPS: 0.459 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/generated_LSGAN_128_128_ds - hecktor_mask - True.npy')\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BRATS - AHVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 15.582 ↑\n",
      "    SSIM: 0.647 ↑\n",
      "    FID: 94.679 ↓\n",
      "    IS: (mean) 1.569 (std) 0.054 : 20.587 ↑\n",
      "    LPIPS: 0.227 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/generated_AHVAE_128_128_ds - brats_mask - True.npy')\n",
    "fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brats - VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 15.181 ↑\n",
      "    SSIM: 0.644 ↑\n",
      "    FID: 106.276 ↓\n",
      "    IS: (mean) 1.730 (std) 0.144 : 14.853 ↑\n",
      "    LPIPS: 0.273 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/generated_VAE_128_128_ds=brats, mask=True.npy')\n",
    "fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HVAE BRATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 15.336 ↑\n",
      "    SSIM: 0.655 ↑\n",
      "    FID: 124.750 ↓\n",
      "    IS: (mean) 1.312 (std) 0.057 : 15.542 ↑\n",
      "    LPIPS: 0.248 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/generated_HVAE_128_128_ds - brats, mask - True, dt - 0.1.npy')\n",
    "fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSGAN BRATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 500 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 14.579 ↑\n",
      "    SSIM: 0.442 ↑\n",
      "    FID: 158.289 ↓\n",
      "    IS: (mean) 1.856 (std) 0.066 : 24.832 ↑\n",
      "    LPIPS: 0.473 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_images = np.load('data/generated_LSGAN_128_128_ds - brats_mask - True.npy')\n",
    "fake_images = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images[:500].to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRATS - dHVAE old training with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 15.614 ↑\n",
      "    SSIM: 0.647 ↑\n",
      "    FID: 92.488 ↓\n",
      "    IS: (mean) 1.582 (std) 0.052 : 21.427 ↑\n",
      "    LPIPS: 0.227 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/dHVAE-BRATS (original).npy')\n",
    "fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dHVAE - BRATS (0.01) = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 13.795 ↑\n",
      "    SSIM: 0.578 ↑\n",
      "    FID: 123.964 ↓\n",
      "    IS: (mean) 1.719 (std) 0.086 : 19.076 ↑\n",
      "    LPIPS: 0.283 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/dHVAE-BRATS (0.05).npy')\n",
    "fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRATS - dHVAE (0.001) = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 15.409 ↑\n",
      "    SSIM: 0.632 ↑\n",
      "    FID: 126.503 ↓\n",
      "    IS: (mean) 1.707 (std) 0.055 : 23.470 ↑\n",
      "    LPIPS: 0.233 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/dHVAE-BRATS (0.05).npy')\n",
    "# fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dHVAE - BRATS (0.1) = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 15.208 ↑\n",
      "    SSIM: 0.633 ↑\n",
      "    FID: 147.453 ↓\n",
      "    IS: (mean) 1.671 (std) 0.049 : 24.154 ↑\n",
      "    LPIPS: 0.235 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/dHVAE-BRATS (0.1).npy')\n",
    "# fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dHVAE - BRATS (0.01) Fethi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 15.622 ↑\n",
      "    SSIM: 0.627 ↑\n",
      "    FID: 88.299 ↓\n",
      "    IS: (mean) 1.817 (std) 0.075 : 22.525 ↑\n",
      "    LPIPS: 0.229 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/dHVAE-BRATS (0.01) Fethi.npy')\n",
    "# fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HVAE + Percep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n",
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 14.923 ↑\n",
      "    SSIM: 0.637 ↑\n",
      "    FID: 104.767 ↓\n",
      "    IS: (mean) 1.600 (std) 0.052 : 21.715 ↑\n",
      "    LPIPS: 0.225 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/HVAE-Percep-BRATS.npy')\n",
    "# fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HVAE + Disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Test metrics (on test set)\n",
      "Evaluation on 838 real/fake/recon images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aghiles/anaconda3/envs/work/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    --------------------------------------------\n",
      "    Total loss (ELBO): 0.000 ↓\n",
      "    Mean squared error: 0.000 ↓\n",
      "    Kullback-Leibler divergence: 0.000 ↓\n",
      "    PSNR: 15.102 ↑\n",
      "    SSIM: 0.633 ↑\n",
      "    FID: 147.954 ↓\n",
      "    IS: (mean) 1.681 (std) 0.085 : 18.456 ↑\n",
      "    LPIPS: 0.237 ↓\n",
      "    kwargs: {}\n",
      "    --------------------------------------------\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "fake_data = np.load('data/HVAE-Disc-Brats.npy')\n",
    "# fake_data = transforms.RandomHorizontalFlip(p=1.0)(torch.from_numpy(fake_data)).numpy()\n",
    "fake_images = fake_data[:test_images.shape[0], 0, None]\n",
    "# min max norm to 0-1\n",
    "for idx in range(len(fake_images)):\n",
    "    fake_images[idx] = (fake_images[idx] - fake_images[idx].min()) / (fake_images[idx].max() - fake_images[idx].min())\n",
    "\n",
    "fake_images = torch.from_numpy(fake_images).type(torch.float32).to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "stats = evaluate_samples(test_images, fake_images, verbose=True, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fae847446f25d1d5cccaa632528b81cb53ce4d6408a7df79225531d1adf33a86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
